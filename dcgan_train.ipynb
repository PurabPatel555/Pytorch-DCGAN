{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtZm6055IwYMXvzNv8M5o0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurabPatel555/Pytorch-DCGAN/blob/master/dcgan_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3snClhEoZ7qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsCW3UltR-de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YRyy8_4eUxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Settings and Hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(\"using gpu\")\n",
        "else:\n",
        "  print(\"using cpu\")\n",
        "\n",
        "d_depth = 16\n",
        "g_depth = 16\n",
        "lr = 0.001\n",
        "bs = 4\n",
        "epochs = 10\n",
        "img_size = 64\n",
        "img_channels = 1\n",
        "seed_channels = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYl_7aCVHNkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----Create the Generator and Discriminator Networks-----\n",
        "\n",
        "class GNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(seed_channels, g_depth * 16, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(g_depth * 16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(g_depth * 16, g_depth * 8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(g_depth * 8),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(g_depth * 8, g_depth * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(g_depth * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(g_depth * 4, g_depth * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(g_depth * 2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(g_depth * 2, img_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),  #Tanh activation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class DNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(img_channels, d_depth, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(d_depth, d_depth * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_depth * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(d_depth * 2, d_depth * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_depth * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(d_depth * 4, d_depth * 8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_depth * 8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        \n",
        "            nn.Conv2d(d_depth * 8, 1, kernel_size=4, stride=2, padding=0),  #We avoid using a FC layer \n",
        "            nn.Sigmoid(), #Sigmoid activation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZS6OcVqlHWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----Training the Networks-----\n",
        "#Transforms\n",
        "all_transforms = transforms.Compose([\n",
        "  transforms.Grayscale(num_output_channels=1),  #Convert to grayscale \n",
        "  transforms.Resize((img_size, img_size)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5),(0.5)),  #This should be changed to the mean and std of your dataset                                    \n",
        "])\n",
        "\n",
        "#Dataset\n",
        "root_dir = '/content/drive/My Drive/dcgan/animeface-character-dataset'\n",
        "dataset = datasets.ImageFolder(root = root_dir, transform = all_transforms)\n",
        "\n",
        "#Dataloader\n",
        "loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "#Initialize the networks\n",
        "G = GNet().to(device)  #Generator\n",
        "D = DNet().to(device) #Discriminator\n",
        "G.train()\n",
        "D.train()\n",
        "\n",
        "#Loss and Optimizers\n",
        "criterion = nn.BCELoss()\n",
        "GOptim = optim.Adam(G.parameters(), lr=lr, betas=(0.7,0.999))\n",
        "DOptim = optim.Adam(D.parameters(), lr=lr, betas=(0.7,0.999))\n",
        "\n",
        "#Set a sample seed that will allow us to visualize results as training progresses\n",
        "sample_seed = torch.randn(bs, seed_channels, 1, 1).to(device)\n",
        "\n",
        "#Training\n",
        "for epoch in range(epochs):\n",
        "    loop = tqdm(enumerate(loader), total = len(loader), leave=False)\n",
        "    for batch, (data, targets) in loop:\n",
        "\n",
        "        data = data.to(device)\n",
        "        bs = data.shape[0]\n",
        "\n",
        "        D.zero_grad()\n",
        "        targets = (torch.ones(bs)*0.95).to(device)\n",
        "        results = D(data).reshape(-1)\n",
        "        D_loss_1 = criterion(results, targets)\n",
        "        seeds = torch.randn(bs, seed_channels, 1, 1).to(device)\n",
        "        gen_images = G(seeds)\n",
        "        targets = (torch.ones(bs)*0.05).to(device)\n",
        "        results = D(gen_images.detach()).reshape(-1)\n",
        "        D_loss_0 = criterion(results, targets)\n",
        "        lossD = D_loss_1 + D_loss_0\n",
        "        lossD.backward()\n",
        "        DOptim.step() \n",
        "\n",
        "        G.zero_grad()\n",
        "        targets = torch.ones(bs).to(device)\n",
        "        results = D(gen_images).reshape(-1)\n",
        "        lossG = criterion(results, targets)\n",
        "        lossG.backward()\n",
        "        GOptim.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:  #Show samples at every 100th batch\n",
        "          with torch.no_grad():\n",
        "              loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
        "              loop.set_postfix(lossG = lossG.item())\n",
        "              fake = G(sample_seed)\n",
        "              samples_real = torchvision.utils.make_grid(data, normalize=True)\n",
        "              samples_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "\n",
        "              f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "              ax1.imshow(samples_fake.cpu().numpy().transpose(1,2,0))\n",
        "              ax1.set_title('Fake')\n",
        "              ax2.imshow(samples_real.cpu().numpy().transpose(1,2,0))\n",
        "              ax2.set_title('Real')\n",
        "              plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}